@online{20TezisovPro,
  title = {20 тезисов про войну и мир},
  url = {https://les.media/articles/762502-20-tezisov-pro-voynu-i-mir-for-landing},
  urldate = {2023-03-08},
  abstract = {Возможна ли сейчас в России антивоенная и патриотическая позиция одновременно},
  langid = {russian},
  keywords = {ukraine,war},
  file = {/home/razor/Zotero/storage/VT99N3W9/762502-20-tezisov-pro-voynu-i-mir-for-landing.html}
}

@video{aleksandrshtefanovVoynaUkrainoyPrichiny2023,
  entrysubtype = {video},
  title = {Война с {{Украиной}}: Причины, Ход, Результаты. {{Взгляд}} Из {{России}}},
  shorttitle = {Война с {{Украиной}}},
  editor = {{Александр Штефанов}},
  editortype = {director},
  date = {2023},
  url = {https://www.youtube.com/watch?v=4a9gFN8gcj8},
  urldate = {2023-07-02},
  abstract = {Для желающих поддержать выход новых роликов: https://boosty.to/alexandrshtefanov  https://www.patreon.com/alexandrshtef... 5536 9138 9184 0197 - Тинькофф 2200 2404 4103 3244 - Втб 410011724159658 - Юмани Биткоин: bc1qehzg5jwe2hswn06pd25csjlvxq6ql97wxfwk5c Эфир: 0xDC89d54f752D2f150d2ef393868E025F81856019 USDT (trс 20): TFo5VdcGrEcDCaM2TkkSdUyEfKH6hkNn78 Больше интересных исторических фактов в Telegram канале - https://t.me/alexandrshtefanov 00:00 - У вас есть причины не любить Украину 06:19 - С чего началась СВО? 11:22 - Где вы были 8 лет? 13:46 - Почему Путин не забрал Донбасс? 18:09 - Мобилизация в ЛДНР 25:25 - Аллея ангелов называет Путина украинским националистом 30:15 - Что СВО дала Донбассу? 31:42 - НАТО хотела напасть на Россию? 34:34 - Украина хотела напасть на Россию? 36:40 - Причины и цели войны с Украиной 50:07 - Как Кремль разрушил территориальную целостность РФ? 52:41 - Крепкой руки в стране нет. Вертикали власти тоже. А что есть? 57:10 - Спецоперация полностью провалена?  1:06:10 - За что мы сейчас воюем? 1:06:44 - Что если Россия проиграет? 1:11:59 - Неизбежный финал}
}

@online{alharthiXLSTMTimeLongTermTime2024,
  title = {{{xLSTMTime}}: {{Long-Term Time Series Forecasting With xLSTM}}},
  shorttitle = {{{xLSTMTime}}},
  author = {Alharthi, Musleh and Mahmood, Ausif},
  date = {2024-07-16},
  doi = {10.20944/preprints202407.1246.v1},
  url = {https://www.preprints.org/manuscript/202407.1246/v1},
  urldate = {2024-07-18},
  abstract = {In recent years, transformer-based models have gained prominence in multivariate long-term time series forecasting (LTSF), demonstrating significant advancements despite facing challenges such as high computational demands, difficulty in capturing temporal dynamics, and managing long-term dependencies. The emergence of LTSF-Linear, with its straightforward linear architecture, has notably outperformed transformer-based counterparts, prompting a reevaluation of the transformer's utility in time series forecasting. In response, this paper presents an adaptation of a recent architecture termed extended LSTM (xLSTM) for LTSF. xLSTM incorporates exponential gating and a revised memory structure with higher capacity that has good potential for LTSF. Our adopted architecture for LTSF termed as xLSTMTime surpasses current approaches. We compare xLSTMTime's performance against various state-of-the-art models across multiple real-world datasets, demonstrating superior forecasting capabilities. Our findings suggest that refined recurrent architectures can offer competitive alternatives to transformer-based models in LTSF tasks, potentially redefining the landscape of time series forecasting.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {neural_network,RNN},
  file = {/home/razor/Zotero/storage/MMHBK3DN/Alharthi and Mahmood - 2024 - xLSTMTime Long-Term Time Series Forecasting With .pdf}
}

@online{alizadehLLMFlashEfficient2023,
  title = {{{LLM}} in a Flash: {{Efficient Large Language Model Inference}} with {{Limited Memory}}},
  shorttitle = {{{LLM}} in a Flash},
  author = {Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C. and Rastegari, Mohammad and Farajtabar, Mehrdad},
  date = {2023-12-12},
  eprint = {2312.11514},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.11514},
  urldate = {2024-01-06},
  abstract = {Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, “windowing” strategically reduces data transfer by reusing previously activated neurons, and second, “row-column bundling”, tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {llm,neural_network,optimization},
  file = {/home/razor/Zotero/storage/A3PWGTSR/Alizadeh et al. - 2023 - LLM in a flash Efficient Large Language Model Inf.pdf}
}

@online{AnnotatedBibliographyTemplate,
  title = {Annotated {{Bibliography Template}}},
  url = {https://orgmode.org/worg/exporters/anno-bib-template-worg.html},
  urldate = {2023-01-18},
  keywords = {emacs,org-mode},
  file = {/home/razor/Zotero/storage/CP8JX8M6/anno-bib-template-worg.html}
}

@online{AnonimnyyMessendzherObyazatelnyy2024,
  title = {Анонимный мессенджер — обязательный стандарт для каждого человека},
  date = {2024-11-07},
  url = {https://habr.com/ru/articles/851866/},
  urldate = {2024-11-07},
  abstract = {В наше время важность анонимности в интернете возросла многократно. Интернет уже не то место, где можно спокойно себя чувствовать, сегодня за лайк или неосторожную фразу в чате может прилететь...},
  langid = {russian},
  organization = {Хабр},
  file = {/home/razor/Zotero/storage/9IACX6A8/851866.html}
}

@online{baiConstitutionalAIHarmlessness2022,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  date = {2022-12-15},
  eprint = {2212.08073},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.08073},
  urldate = {2023-02-02},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through selfimprovement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as ‘Constitutional AI’. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use ‘RL from AI Feedback’ (RLAIF). As a result we are able to train a harmless but nonevasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {computer_science,neural_network},
  file = {/home/razor/Zotero/storage/BKLIXUNQ/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf}
}

@online{beckGrokkingEdgeLinear2024,
  title = {Grokking at the {{Edge}} of {{Linear Separability}}},
  author = {Beck, Alon and Levi, Noam and Bar-Sinai, Yohai},
  date = {2024-10-06},
  eprint = {2410.04489},
  eprinttype = {arXiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2410.04489},
  urldate = {2024-10-16},
  abstract = {We study the generalization properties of binary logistic classification in a simplified setting, for which a "memorizing" and "generalizing" solution can always be strictly defined, and elucidate empirically and analytically the mechanism underlying Grokking in its dynamics. We analyze the asymptotic long-time dynamics of logistic classification on a random feature model with a constant label and show that it exhibits Grokking, in the sense of delayed generalization and non-monotonic test loss. We find that Grokking is amplified when classification is applied to training sets which are on the verge of linear separability. Even though a perfect generalizing solution always exists, we prove the implicit bias of the logisitc loss will cause the model to overfit if the training data is linearly separable from the origin. For training sets that are not separable from the origin, the model will always generalize perfectly asymptotically, but overfitting may occur at early stages of training. Importantly, in the vicinity of the transition, that is, for training sets that are almost separable from the origin, the model may overfit for arbitrarily long times before generalizing. We gain more insights by examining a tractable one-dimensional toy model that quantitatively captures the key features of the full model. Finally, we highlight intriguing common properties of our findings with recent literature, suggesting that grokking generally occurs in proximity to the interpolation threshold, reminiscent of critical phenomena often observed in physical systems.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {machine_learning,neural_network},
  file = {/home/razor/Zotero/storage/Z44EZ96J/Beck et al. - 2024 - Grokking at the Edge of Linear Separability.pdf}
}

@online{bidermanLoRALearnsLess2024,
  title = {{{LoRA Learns Less}} and {{Forgets Less}}},
  author = {Biderman, Dan and Ortiz, Jose Gonzalez and Portes, Jacob and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and Blakeney, Cody and Cunningham, John P.},
  date = {2024-05-15},
  eprint = {2405.09673},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2405.09673},
  urldate = {2024-06-24},
  abstract = {Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (≈100K prompt-response pairs) and continued pretraining (≈10B unstructured tokens) data regimes. Our results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model’s performance on tasks outside the target domain. We show that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. We show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {fine_tuning,llm},
  file = {/home/razor/Zotero/storage/NKIYI69C/Biderman et al. - 2024 - LoRA Learns Less and Forgets Less.pdf}
}

@article{blumFoundationsDataScience,
  title = {Foundations of {{Data Science}}},
  author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
  langid = {english},
  keywords = {computer_science,data_science},
  file = {/home/razor/Zotero/storage/YEUD637K/Blum et al. - Foundations of Data Science.pdf}
}

@online{borrettiEffectiveSpacedRepetition2023,
  title = {Effective {{Spaced Repetition}}},
  author = {Borretti, Fernando},
  date = {2023-04-10T00:00:00+00:00},
  url = {https://borretti.me/article/effective-spaced-repetition},
  urldate = {2023-04-10},
  abstract = {“To flash conviction on the mind.”},
  langid = {english},
  organization = {Fernando Borretti},
  keywords = {srs},
  file = {/home/razor/Zotero/storage/VI2CZI8Q/effective-spaced-repetition.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2025-02-03},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/TDTTVPCE/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@online{CanCatalinEye2014,
  title = {Can {{Catalin Eye Drops Protect Eyes From Cataracts}}?},
  year = {2/21/2014 6:45:00 PM},
  url = {https://www.aao.org/eye-health/ask-ophthalmologist-q/can-catalin-eye-drops-protect-eyes-from-cataracts},
  urldate = {2023-01-31},
  abstract = {Is it true that Catalin eye drops can protect eyes from cataracts?},
  langid = {english},
  organization = {American Academy of Ophthalmology},
  keywords = {medicine},
  file = {/home/razor/Zotero/storage/5P9GPBVH/can-catalin-eye-drops-protect-eyes-from-cataracts.html}
}

@video{chamade!EvromaydanBezMifov2023,
  entrysubtype = {video},
  title = {Евромайдан Без Мифов и Пропаганды},
  editor = {{Chamade!}},
  editortype = {director},
  date = {2023},
  url = {https://www.youtube.com/watch?v=sVM69vcvwmw},
  urldate = {2023-01-29},
  keywords = {politics,ukraine}
}

@online{chaoJailbreakingBlackBox2023,
  title = {Jailbreaking {{Black Box Large Language Models}} in {{Twenty Queries}}},
  author = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J. and Wong, Eric},
  date = {2023-10-13},
  eprint = {2310.08419},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.08419},
  urldate = {2024-02-09},
  abstract = {There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR—which is inspired by social engineering attacks—uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {llm,neural_network,security},
  file = {/home/razor/Zotero/storage/KAXM48IX/Chao et al. - 2023 - Jailbreaking Black Box Large Language Models in Tw.pdf}
}

@online{chichkovKakPoluchitRVP2022,
  title = {Как Получить {{РВП}} в {{Казахстане}}? {{Подробно}} о Самом Популярном Способе Оставаться Дольше 90 Дней — {{Миграция}} На vc.Ru},
  shorttitle = {Как Получить {{РВП}} в {{Казахстане}}?},
  author = {Чичков, Александр},
  date = {2022-12-07T10:58:59+03:00},
  url = {https://vc.ru/migrate/556423-kak-poluchit-rvp-v-kazahstane-podrobno-o-samom-populyarnom-sposobe-ostavatsya-dolshe-90-dney},
  urldate = {2023-03-10},
  abstract = {РВП — разрешение на временное проживание, выдаваемое миграционной службой РК. Даёт право находиться в Казахстане определенный срок в зависимости от основания его получения. Является самым распространенным способом легализации в стране.},
  organization = {vc.ru},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/XWDFRWHV/556423-kak-poluchit-rvp-v-kazahstane-podrobno-o-samom-populyarnom-sposobe-ostavatsya-dolshe-90-.html}
}

@online{CitationsOrgmodeOrgcite,
  title = {Citations in Org-Mode: {{Org-cite}} and {{Citar}}},
  shorttitle = {Citations in Org-Mode},
  url = {https://kristofferbalintona.me/posts/202206141852/},
  urldate = {2023-01-18},
  abstract = {Musings},
  langid = {american},
  organization = {Kristoffer Balintona},
  keywords = {emacs,org-mode},
  file = {/home/razor/Zotero/storage/SMCA43AV/202206141852.html}
}

@online{dingLongRoPEExtendingLLM2024,
  title = {{{LongRoPE}}: {{Extending LLM Context Window Beyond}} 2 {{Million Tokens}}},
  shorttitle = {{{LongRoPE}}},
  author = {Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  date = {2024-02-21},
  eprint = {2402.13753},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.13753},
  urldate = {2024-03-10},
  abstract = {Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {llm},
  file = {/home/razor/Zotero/storage/CUWDP2XV/Ding et al. - 2024 - LongRoPE Extending LLM Context Window Beyond 2 Mi.pdf}
}

@article{erkokValueRecursionMonadic,
  title = {Value {{Recursion}} in {{Monadic Computations}}},
  author = {Erkok, Levent},
  langid = {english},
  keywords = {computer_science,haskell},
  file = {/home/razor/Zotero/storage/DK5TIKUH/Erkok - Value Recursion in Monadic Computations.pdf}
}

@online{FashizmIliNeofashizm2023,
  title = {Фашизм или неофашизм? Как бороться с диктатурами в информационную эру | Altleft | Альтернативные левые},
  shorttitle = {Фашизм или неофашизм?},
  date = {2023-01-22T15:04:21+00:00},
  url = {https://altleft.org/2023/01/22/fashizm-ili-neofashizm-kak-borotsja-s-diktaturami-v-informacionnuju-jeru/},
  urldate = {2023-01-23},
  abstract = {Рассказываем, почему политический режим в России — это не фашизм, а неофашизм. Чем различаются правые диктатуры XX и XXI веков? На примере России пробуем разобраться в этих вопросах и предлагаем несколько идей для поиска новых технологий борьбы с авторитаризмом.},
  langid = {russian},
  keywords = {politics},
  file = {/home/razor/Zotero/storage/7QY86HH8/fashizm-ili-neofashizm-kak-borotsja-s-diktaturami-v-informacionnuju-jeru.html}
}

@inreference{GeologiyaArmenii2023,
  title = {Геология Армении},
  booktitle = {Википедия},
  date = {2023-01-18T18:01:42Z},
  url = {https://ru.wikipedia.org/w/index.php?title=%D0%93%D0%B5%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%8F_%D0%90%D1%80%D0%BC%D0%B5%D0%BD%D0%B8%D0%B8&oldid=127952986},
  urldate = {2023-03-04},
  abstract = {С точки зрения геологической структуры, Армения составляет часть большого сводчатого сгиба Закавказья и Среднеараксийской межгорной низменности. Эти две геологические структурные единицы входят в состав Кавказ-Анатоль-Иранского сегмента Средиземноморского складчатого пояса. Исходя из времени основания геологических структурных единиц и возраста окончания складкообразования, на территории Армении выделяются: Сомхето-Капанский комплекс, Базум-Зангезурский и Приараксийский пояса.},
  langid = {russian},
  keywords = {emigration,geology},
  annotation = {Page Version ID: 127952986},
  file = {/home/razor/Zotero/storage/VR4BLIQB/Геология_Армении.html}
}

@article{gillenTwelveWeeksSprint2016,
  title = {Twelve {{Weeks}} of {{Sprint Interval Training Improves Indices}} of {{Cardiometabolic Health Similar}} to {{Traditional Endurance Training}} despite a {{Five-Fold Lower Exercise Volume}} and {{Time Commitment}}},
  author = {Gillen, Jenna B. and Martin, Brian J. and MacInnis, Martin J. and Skelly, Lauren E. and Tarnopolsky, Mark A. and Gibala, Martin J.},
  date = {2016},
  journaltitle = {PloS One},
  shortjournal = {PLoS One},
  volume = {11},
  number = {4},
  eprint = {27115137},
  eprinttype = {pmid},
  pages = {e0154075},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0154075},
  abstract = {AIMS: We investigated whether sprint interval training (SIT) was a time-efficient exercise strategy to improve insulin sensitivity and other indices of cardiometabolic health to the same extent as traditional moderate-intensity continuous training (MICT). SIT involved 1 minute of intense exercise within a 10-minute time commitment, whereas MICT involved 50 minutes of continuous exercise per session. METHODS: Sedentary men (27±8y; BMI = 26±6kg/m2) performed three weekly sessions of SIT (n = 9) or MICT (n = 10) for 12 weeks or served as non-training controls (n = 6). SIT involved 3x20-second 'all-out' cycle sprints (\textasciitilde 500W) interspersed with 2 minutes of cycling at 50W, whereas MICT involved 45 minutes of continuous cycling at \textasciitilde 70\% maximal heart rate (\textasciitilde 110W). Both protocols involved a 2-minute warm-up and 3-minute cool-down at 50W. RESULTS: Peak oxygen uptake increased after training by 19\% in both groups (SIT: 32±7 to 38±8; MICT: 34±6 to 40±8ml/kg/min; p{$<$}0.001 for both). Insulin sensitivity index (CSI), determined by intravenous glucose tolerance tests performed before and 72 hours after training, increased similarly after SIT (4.9±2.5 to 7.5±4.7, p = 0.002) and MICT (5.0±3.3 to 6.7±5.0 x 10-4 min-1 [μU/mL]-1, p = 0.013) (p{$<$}0.05). Skeletal muscle mitochondrial content also increased similarly after SIT and MICT, as primarily reflected by the maximal activity of citrate synthase (CS; P{$<$}0.001). The corresponding changes in the control group were small for VO2peak (p = 0.99), CSI (p = 0.63) and CS (p = 0.97). CONCLUSIONS: Twelve weeks of brief intense interval exercise improved indices of cardiometabolic health to the same extent as traditional endurance training in sedentary men, despite a five-fold lower exercise volume and time commitment.},
  langid = {english},
  pmcid = {PMC4846072},
  keywords = {medicine},
  file = {/home/razor/Zotero/storage/HJL57V4B/gillen2016.pdf.pdf;/home/razor/Zotero/storage/KAP3GX5U/Gillen et al. - 2016 - Twelve Weeks of Sprint Interval Training Improves .pdf}
}

@online{GuideIPLayer,
  title = {Guide to {{IP Layer Network Administration}} with {{Linux}}},
  url = {http://linux-ip.net/linux-ip/linux-ip-single.html#ch-routing},
  urldate = {2023-04-09},
  keywords = {linux,networking,routing},
  file = {/home/razor/Zotero/storage/7GP4RBHI/linux-ip-single.html}
}

@online{hayouLoRAEfficientLow2024,
  title = {{{LoRA}}+: {{Efficient Low Rank Adaptation}} of {{Large Models}}},
  shorttitle = {{{LoRA}}+},
  author = {Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  date = {2024-02-19},
  eprint = {2402.12354},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2402.12354},
  urldate = {2024-06-24},
  abstract = {In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in [17] leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen fixed ratio. We call this proposed algorithm LoRA+. In our extensive experiments, LoRA+ improves performance (1\% − 2\% improvements) and finetuning speed (up to ∼ 2X SpeedUp), at the same computational cost as LoRA.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {fine_tuning,llm},
  file = {/home/razor/Zotero/storage/M9DTY9UM/Hayou et al. - 2024 - LoRA+ Efficient Low Rank Adaptation of Large Mode.pdf}
}

@article{henryModularizingGHC,
  title = {Modularizing {{GHC}}},
  author = {Henry, Sylvain and Ericson, John and Young, Jeffrey M},
  abstract = {GHC is the de facto main implementation of the Haskell programming language. Over its year history it has served well the needs of pure functional programmers and researchers alike. However, GHC is not exemplary of good large scale system design in a pure function language. Rather ironically, it violates the properties that draw people to functional programming in the first place: immutability, modularity, and composability. These scars have become more noticeable as modern projects currently underway, such as the Haskell Language Server and cross-compilation, aim to fulfill user needs and desires far more diverse than before. We believe a better GHC is possible. We write this paper to properly situate both the current state of GHC’s codebase and that better future state in the design space of large scale, pure, functional systems. Firstly, we document in detail, GHC’s architectural problems, such as low coherence and high coupling of mutable state, and their genesis. Secondly, we describe what we believe to be a superior design, drawing heavily on domain-driven design principles. Lastly, we sketch a plan to get this design implemented iteratively and durably, mentioning interactions with other ongoing refactorings (structured errors, Trees That Grow, etc.). All of this is informed not just by our own experience working on GHC and deep dives into its history, but also by the traditional software engineering literature. The paper is written from an engineering perspective, with the hope that our collection and recapitulation may provide insight into future best practices for other pure functional software engineers.},
  langid = {english},
  keywords = {ghc,haskell},
  file = {/home/razor/Zotero/storage/R37X49SK/Henry et al. - Modularizing GHC.pdf}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1503.02531},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2025-02-03},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/8GYYLGUR/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11-15},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  urldate = {2023-01-30},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  keywords = {computer_science,neural_network},
  file = {/home/razor/Zotero/storage/YA5H2Q5M/hochreiter1997.pdf.pdf;/home/razor/Zotero/storage/JUKCZAES/Long-Short-Term-Memory.html}
}

@online{honnefMyOrgroamWorkflows2023,
  title = {My Org-Roam Workflows for Taking Notes and Writing Articles},
  author = {Honnef, Dominik},
  date = {2023-01-12},
  url = {https://honnef.co/articles/my-org-roam-workflows-for-taking-notes-and-writing-articles/},
  urldate = {2023-01-18},
  langid = {english},
  organization = {Dominik Honnef's website},
  keywords = {emacs,org-mode},
  file = {/home/razor/Zotero/storage/3WPJ4X4L/my-org-roam-workflows-for-taking-notes-and-writing-articles.html}
}

@online{HttpsProceedingsNeurips,
  title = {{{https://proceedings.neurips.cc/paper\_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf}}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  urldate = {2024-06-19},
  keywords = {llm,prompt_engineering},
  file = {/home/razor/Zotero/storage/DGD5PPBA/httpsproceedings.neurips.ccpaper_filespaper2.pdf}
}

@online{huangGamePadLearningEnvironment2018,
  title = {{{GamePad}}: {{A Learning Environment}} for {{Theorem Proving}}},
  shorttitle = {{{GamePad}}},
  author = {Huang, Daniel and Dhariwal, Prafulla and Song, Dawn and Sutskever, Ilya},
  date = {2018-12-21},
  eprint = {1806.00608},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1806.00608},
  url = {http://arxiv.org/abs/1806.00608},
  urldate = {2023-04-11},
  abstract = {In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.},
  pubstate = {prepublished},
  keywords = {neural_network,openai,theorem_prooving},
  file = {/home/razor/Zotero/storage/473R29P6/Huang et al. - 2018 - GamePad A Learning Environment for Theorem Provin.pdf;/home/razor/Zotero/storage/9WSPKLTX/1806.html}
}

@online{huChainofSymbolPromptingElicits2023,
  title = {Chain-of-{{Symbol Prompting Elicits Planning}} in {{Large Langauge Models}}},
  author = {Hu, Hanxu and Lu, Hongyuan and Zhang, Huajian and Song, Yun-Ze and Lam, Wai and Zhang, Yue},
  date = {2023-10-03},
  eprint = {2305.10276},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.10276},
  urldate = {2024-06-20},
  abstract = {While conventional Chain-of-Thought prompting shows promising performance on various language tasks for LLMs, the spatial scenarios are nearly unexplored. In this paper, we first investigate the performance of LLMs on complex spatial understanding and planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act or reason correspondingly in text. By evaluating on classic spatial planning scenarios, we found that current popular LLMs such as ChatGPT still lack abilities to handle spatial relationships in texts. This raises a question: Is the natural language the best way to represent complex spatial environments for LLMs, or are other alternatives such as symbolic representations more efficient and effective for LLMs? To this end, we propose a novel method called COS (Chain-of-Symbol Prompting) that represents the spatial relationships with condensed symbols during the chained intermediate thinking steps. COS is easy to use and does not need additional training on LLMs. Extensive experiments indicate that COS clearly surpasses the performance of the Chainof-Thought (CoT) Prompting described in natural language in all three spatial planning tasks and existing spatial QA benchmark, with even fewer tokens used in the inputs compared with CoT. The performance gain is strong, by up to 60.8\% accuracy (from 31.8\% to 92.6\%) on Brick World for ChatGPT. COS also reduces the number of tokens in the prompt obviously, by up to 65.8\% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {llm,prompt_engineering},
  file = {/home/razor/Zotero/storage/PREE6WXC/Hu et al. - 2023 - Chain-of-Symbol Prompting Elicits Planning in Larg.pdf}
}

@online{igrushkiwroteSkolkoLetSSSR,
  title = {Сколько Лет в {{СССР}} Стояли в Очереди На Жильё?},
  author = {детям не {игрушкиwrote}, Cпички and детям не игрушки {burckina\_faso}, Cпички},
  url = {https://burckina-faso.livejournal.com/3765874.html},
  urldate = {2024-01-23},
  abstract = {Сейчас усиленно продвигается черный миф о том, что в СССР вообще бесплатного жилья не было. Мол, оно ведь не давалось в собственность гражданам, а значит не было бесплатным. Логика в стиле в огороде бузина, а в Киеве дядька. Или вам шашечки или ехать? То есть люди намерено…},
  langid = {american},
  file = {/home/razor/Zotero/storage/88D3BEE4/3765874.html}
}

@video{kakteper/proektovd-infoKAKTEPERLzhenauka2023,
  entrysubtype = {video},
  title = {{{КАК ТЕПЕРЬ}} Лженаука Прочно Обосновалась Во Власти и Обществе {{России}} / {{Александр Панчин}}},
  editor = {{Как теперь / проект ОВД-Инфо}},
  editortype = {director},
  date = {2023},
  url = {https://www.youtube.com/watch?v=i_RuBEPonBg},
  urldate = {2023-04-14},
  abstract = {Как лженаука и конспирология становится частью политики России? Почему власти верят в такое количество теорий заговора: от генетического оружия до «кругом враги»? Как все это влияет на общественное мнение и свободу? Что будет с наукой в России дальше из-за изоляции и эмиграции ученых? Новый гость «Как теперь» от ОВД-Инфо —~биолог, писатель, популяризатор науки и член комиссии РАН по Борье с лженаукой Александр Панчин.  Ютуб-канал Александра Панчина: ~~~/~@alexanderpanchin...~~ Лекции и соцсети Панчина: https://scinquisitor.taplink.ws Если вас задержали на акции Пишите: https://t.me/OvdInfoBot  Звоните: 8 800 707-05-28 Все соцсети ОВД-Инфо: https://ovdinfo.taplink.ws  00:00 Биолог Александр Панчин 00:56~Почему людям так легко поверить в теории заговора? 04:26~Как лженаука и конспирология встраиваются в политику России? 10:37~Есть ли в российской власти люди, которые верят в конспирологические теории? 14:45~Как лженаука влияет на общественное мнение в России? 15:53~Война усилила склонность людей верить в конспирологию? 20:48~Боится ли российская власть ученых?~ 24:02~Много ли ученых вернулись в Россию после эмиграции в начале войны? 26:11~Как изоляция России скажется на развитии науки? 28:59~Будут ли чаще транслировать псевдонаучные идеи на государственном уровне? 31:15~Что должно произойти, чтобы ученые начали возвращаться в Россию? 33:11~Что вы сделаете, когда закончится война? \#кактеперь \#овдинфо \#панчин}
}

@online{KakUehatKyrgyzstan,
  title = {Как Уехать в {{Кыргызстан}} и Жить в Нём},
  url = {https://www.aviasales.ru/psgr/article/kyrgyzstan-run},
  urldate = {2023-03-05},
  abstract = {Иссык-Куль в трёх часах езды, нерезиновые овощи и фрукты, а каждые выходные — хайкинг по природным красотам.},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/TI9YXCRH/kyrgyzstan-run.html}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2025-02-08},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/HVV8VX3H/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/home/razor/Zotero/storage/HQ2K8A8M/1312.html}
}

@video{kognitivnyynadzorChemTYLuchshe2023,
  entrysubtype = {video},
  title = {А Чем {{ТЫ}} Лучше {{Кучеры}}?},
  editor = {{Когнитивный надзор}},
  editortype = {director},
  date = {2023},
  url = {https://www.youtube.com/watch?v=L1TmxTzYUaE},
  urldate = {2023-02-07},
  keywords = {cognitive_bias,video}
}

@video{KomuNuzhenTakoy,
  entrysubtype = {video},
  title = {(2) {{Кому}} Нужен Такой {{Центральный Банк}}? {{Рафаэль Абдулов}} // {{Fundamentum}} \#28 - {{YouTube}}},
  url = {https://www.youtube.com/watch?v=k0t8bYCNwcY},
  urldate = {2023-01-17},
  keywords = {economy},
  file = {/home/razor/Zotero/storage/7P8HH8LU/watch.html}
}

@video{kritikakriticheskoykritikiRusskiyNacizmDShRG2023,
  entrysubtype = {video},
  title = {Русский Нацизм - {{ДШРГ}} "{{Русич}}"},
  editor = {{Критика Критической Критики}},
  editortype = {director},
  date = {2023},
  url = {https://www.youtube.com/watch?v=rJT7EtAaJbg},
  urldate = {2023-01-29},
  keywords = {fascism,politics,russian_federation}
}

@article{lindleyPleasurePainDependently,
  title = {The {{Pleasure}} and {{Pain}} of {{Dependently Typed Haskell Programming}}},
  author = {Lindley, Sam and McBride, Conor},
  abstract = {Haskell’s type system has outgrown its Hindley-Milner roots to the extent that it now stretches to the basics of dependently typed programming. In this paper, we collate and classify techniques for programming with dependent types in Haskell, and contribute some new ones. In particular, through extended examples—merge-sort and rectangular tilings—we show how to exploit Haskell’s constraint solver as a theorem prover, delivering code which, as Agda programmers, we envy. We explore the compromises involved in simulating variations on the theme of the dependent function space in an attempt to help programmers put dependent types to work, and to inform the evolving language design both of Haskell and of dependently typed languages more broadly.},
  langid = {english},
  keywords = {computer_science,haskell},
  file = {/home/razor/Zotero/storage/X5MB5EHB/Lindley and McBride - The Pleasure and Pain of Dependently Typed Haskell.pdf}
}

@article{makarFormateAssayBody1975,
  title = {Formate Assay in Body Fluids: Application in Methanol Poisoning},
  shorttitle = {Formate Assay in Body Fluids},
  author = {Makar, A. B. and McMartin, K. E. and Palese, M. and Tephly, T. R.},
  date = {1975-06},
  journaltitle = {Biochemical Medicine},
  shortjournal = {Biochem Med},
  volume = {13},
  number = {2},
  eprint = {1},
  eprinttype = {pmid},
  pages = {117--126},
  issn = {0006-2944},
  doi = {10.1016/0006-2944(75)90147-7},
  langid = {english},
  file = {/home/razor/Zotero/storage/5893AR6F/makar1975.pdf.pdf}
}

@online{MatvienkoZayavilaGotovnosti2023,
  title = {Матвиенко заявила о готовности к переговорам с Украиной без условий},
  date = {2023-02-01},
  url = {https://www.rbc.ru/rbcfreenews/63da5b569a7947f26b8cff86},
  urldate = {2023-02-01},
  abstract = {Россия готова приступить к переговорам с Украиной, если Киев выразит реальную готовность к ним, заявила спикер Совета Федерации Валентина Матвиенко. Наша позиция четкая, внятная и прозрачная. Мы ...},
  langid = {russian},
  organization = {РБК},
  keywords = {russian_federation,war},
  file = {/home/razor/Zotero/storage/TNLVE3NI/63da5b569a7947f26b8cff86.html}
}

@article{mcaleeseLLMCriticsHelp,
  title = {{{LLM Critics Help Catch LLM Bugs}}},
  author = {McAleese, Nat and Pokorny, Michael and Uribe, Juan Felipe Cerón},
  abstract = {Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains “critic” models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63\% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as “flawless”, even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.},
  langid = {english},
  keywords = {fine_tuning,llm,RLHF},
  file = {/home/razor/Zotero/storage/XSXUDSDF/McAleese et al. - LLM Critics Help Catch LLM Bugs.pdf}
}

@video{mikhailvishnevskiyEZhOVIKGREBENChATYYNootrop2022,
  entrysubtype = {video},
  title = {{{ЕЖОВИК ГРЕБЕНЧАТЫЙ}}: Ноотроп, Иммуномодулятор, Косметическое Средство. {{Михаил Вишневский}}},
  shorttitle = {{{ЕЖОВИК ГРЕБЕНЧАТЫЙ}}},
  editor = {{Mikhail Vishnevskiy}},
  editortype = {director},
  date = {2022},
  url = {https://www.youtube.com/watch?v=YruqJtSd5IA},
  urldate = {2023-01-18},
  keywords = {biology}
}

@online{morrisLevelsAGIOperationalizing2023,
  title = {Levels of {{AGI}}: {{Operationalizing Progress}} on the {{Path}} to {{AGI}}},
  shorttitle = {Levels of {{AGI}}},
  author = {Morris, Meredith Ringel and Sohl-dickstein, Jascha and Fiedel, Noah and Warkentin, Tris and Dafoe, Allan and Faust, Aleksandra and Farabet, Clement and Legg, Shane},
  date = {2023-11-04},
  eprint = {2311.02462},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.02462},
  urldate = {2023-11-14},
  abstract = {We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose “Levels of AGI” based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/Z55G9PXQ/Morris et al. - 2023 - Levels of AGI Operationalizing Progress on the Pa.pdf}
}

@inproceedings{mosbachFewshotFinetuningVs2023,
  title = {Few-Shot {{Fine-tuning}} vs. {{In-context Learning}}: {{A Fair Comparison}} and {{Evaluation}}},
  shorttitle = {Few-Shot {{Fine-tuning}} vs. {{In-context Learning}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {Mosbach, Marius and Pimentel, Tiago and Ravfogel, Shauli and Klakow, Dietrich and Elazar, Yanai},
  date = {2023},
  pages = {12284--12314},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.779},
  url = {https://aclanthology.org/2023.findings-acl.779},
  urldate = {2024-06-22},
  eventtitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  langid = {english},
  keywords = {fine_tuning,llm},
  file = {/home/razor/Zotero/storage/IZRPFN94/Mosbach et al. - 2023 - Few-shot Fine-tuning vs. In-context Learning A Fa.pdf}
}

@online{nemecekCoinductiveGuideInductive2023,
  title = {Coinductive Guide to Inductive Transformer Heads},
  author = {Nemecek, Adam},
  date = {2023-02-03},
  eprint = {2302.01834},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.01834},
  url = {http://arxiv.org/abs/2302.01834},
  urldate = {2023-02-28},
  abstract = {We argue that all building blocks of transformer models can be expressed with a single concept: combinatorial Hopf algebra. Transformer learning emerges as a result of the subtle interplay between the algebraic and coalgebraic operations of the combinatorial Hopf algebra. Viewed through this lens, the transformer model becomes a linear time-invariant system where the attention mechanism computes a generalized convolution transform and the residual stream serves as a unit impulse. Attention-only transformers then learn by enforcing an invariant between these two paths. We call this invariant Hopf coherence. Due to this, with a degree of poetic license, one could call combinatorial Hopf algebras "tensors with a built-in loss function gradient". This loss function gradient occurs within the single layers and no backward pass is needed. This is in contrast to automatic differentiation which happens across the whole graph and needs a explicit backward pass. This property is the result of the fact that combinatorial Hopf algebras have the surprising property of calculating eigenvalues by repeated squaring.},
  pubstate = {prepublished},
  keywords = {llm,math,neural_network},
  file = {/home/razor/Zotero/storage/Y5U52ZLA/Nemecek - 2023 - Coinductive guide to inductive transformer heads.pdf;/home/razor/Zotero/storage/439P7S4E/2302.html}
}

@online{NervnayaSistemaKishechnika,
  title = {Нервная Система Кишечника: Энтеральная Нервная Система},
  url = {https://meduniver.com/Medical/Neurology/nervnaia_sistema_kishechnika.html},
  urldate = {2023-01-18},
  keywords = {biology},
  file = {/home/razor/Zotero/storage/T3749DV5/nervnaia_sistema_kishechnika.html}
}

@online{NeuralNetworksZero,
  title = {Neural {{Networks}}: {{Zero To Hero}}},
  url = {https://karpathy.ai/zero-to-hero.html},
  urldate = {2023-01-21},
  keywords = {neural_network},
  file = {/home/razor/Zotero/storage/UIUC9DBR/zero-to-hero.html}
}

@online{novostiRossiyuIzLaosa20190109T0320,
  title = {В Россию из Лаоса привезли тридцать танков Т-34},
  author = {Новости, Р. И. А.},
  year = {20190109T0320},
  url = {https://ria.ru/20190109/1549117666.html},
  urldate = {2023-01-27},
  abstract = {В Россию вернулись 30 танков Т-34, переданных Лаосом. Об этом сообщил департамент информации и массовых коммуникаций Минобороны. РИА Новости, 03.03.2020},
  langid = {russian},
  organization = {РИА Новости},
  file = {/home/razor/Zotero/storage/BW2IEQLN/1549117666.html}
}

@online{OGASMolodyeUchyonye,
  title = {{{ОГАС}} 2.0: Молодые Учёные Разработали {{ПЛАН СПАСЕНИЯ ЭКОНОМИКИ РОССИИ}}/{{При}} Чём Тут {{Глушков}} и {{Макаренко}}? - {{YouTube}}},
  url = {https://www.youtube.com/watch?v=hd2upb9jFV4},
  urldate = {2023-03-03},
  keywords = {economy},
  file = {/home/razor/Zotero/storage/6DV3I92U/watch.html}
}

@online{openaiGPT4TechnicalReport,
  title = {{{GPT-4 Technical Report}}},
  author = {{OpenAI}},
  url = {https://cdn.openai.com/papers/gpt-4.pdf},
  urldate = {2023-03-15},
  langid = {american},
  keywords = {gpt4,llm,neural_network,openai},
  file = {/home/razor/Zotero/storage/6VVUZR2E/gpt-4.pdf}
}

@online{OsipovNaznachilVyplaty2023,
  title = {Осипов назначил выплаты забайкальцам за захват немецких и американских танков на СВО},
  date = {2023-01-30},
  url = {https://www.chita.ru/text/society/2023/01/30/72016034/},
  urldate = {2023-01-30},
  abstract = {За один танк обещают дать 3 миллиона рублей},
  langid = {russian},
  organization = {Chita.ru - новости Читы},
  keywords = {ukraine},
  file = {/home/razor/Zotero/storage/M6Y757NL/72016034.html}
}

@inreference{Paternalizm2022,
  title = {Патернализм},
  booktitle = {Википедия},
  date = {2022-12-18T19:07:31Z},
  url = {https://ru.wikipedia.org/w/index.php?title=%D0%9F%D0%B0%D1%82%D0%B5%D1%80%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%BC&oldid=127337723},
  urldate = {2023-02-08},
  abstract = {Патернали́зм (от лат. paternus — «отцовский, отеческий») — система отношений, при которой власти обеспечивают базовые потребности граждан, а граждане в обмен на это позволяют властям диктовать модели своего поведения, как публичного, так и частного характера.  Патернализм отражает узость перспективы, социальное объединение путём принятия единственного кодекса этики, ограничения интересов и форм опыта теми, которые уже установились как традиционные. Патернализм — система отношений, основанная на покровительстве, опеке и контроле старшими младших (подопечных), а также подчинении младших старшим. Патернализм во внутригосударственных отношениях — принципы и практика государственного управления, построенного по образу контроля государства над людьми (аналогично контролю отца над детьми в патриархальной семье). Патернализм в трудовых отношениях (в некоторых странах) — система дополнительных льгот, субсидий и выплат на предприятиях за счёт предпринимателей с целью закрепления кадров, повышения производительности, смягчения напряжения. Патернализм в международных отношениях — опека крупными государствами более слабых стран, колоний, подопечных территорий.},
  langid = {russian},
  keywords = {politics},
  annotation = {Page Version ID: 127337723},
  file = {/home/razor/Zotero/storage/6YRBM6XX/Патернализм.html}
}

@online{PostPereezdaKyrgyzstan,
  title = {Пост переезда в Кыргызстан. Бишкек.},
  url = {https://vas3k.club/post/14456/},
  urldate = {2023-03-05},
  abstract = {последний UPD поста - 28.09.2022 Чат для тех кто решил понаехать или уже это сделал. Интро Этот вариант может быть полезен тем: у кого слабый англи…},
  langid = {russian},
  organization = {Вастрик.Клуб},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/KAEDHVEJ/14456.html}
}

@online{PravilaPeresecheniyaGranicy,
  title = {Правила пересечения границы},
  url = {https://sahmanapah.sns.am/ru/%D0%BF%D1%80%D0%B0%D0%B2%D0%B8%D0%BB%D0%B0-%D0%BF%D0%B5%D1%80%D0%B5%D1%81%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%86%D1%8B/},
  urldate = {2023-03-04},
  langid = {russian},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/N9P53I5Z/правила-пересечения-границы.html}
}

@video{prekrasnayarossiyaGubernatorIzdevaetsyaNad,
  entrysubtype = {video},
  title = {Губернатор Издевается Над Ветеранами | {{Прекрасная Россия}}},
  editor = {{Прекрасная Россия}},
  editortype = {director},
  url = {https://www.youtube.com/watch?v=a8e8y7gRJW4},
  urldate = {2023-01-29},
  keywords = {memorials,russian_federation}
}

@online{PrigranichnyyKonfliktKirgizii2022,
  title = {Приграничный конфликт Киргизии и Таджикистана. Главное},
  date = {2022-09-18},
  url = {https://www.rbc.ru/politics/18/09/2022/632428299a794745d8046839},
  urldate = {2023-03-09},
  abstract = {Конфликт между странами разгорелся в ночь на 16 сентября. Киргизская сторона 18 сентября сообщила о стабилизации обстановки на границе. Число погибших с обеих сторон к тому моменту достигло 84},
  langid = {russian},
  organization = {РБК},
  keywords = {emigration,war},
  file = {/home/razor/Zotero/storage/6R2Z7FUU/632428299a794745d8046839.html}
}

@video{prostyechislaPrincipyRabotyEtapy2023,
  entrysubtype = {video},
  title = {Принципы Работы и Этапы Развития Экономики {{СССР}}. {{Алексей Сафронов}} // {{Лекция}} в {{МФТИ}} 30.10.2022},
  editor = {{Простые числа}},
  editortype = {director},
  date = {2023},
  url = {https://www.youtube.com/watch?v=3H07URgFB4s},
  urldate = {2023-02-02},
  keywords = {economy,ussr}
}

@online{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022-04-12},
  eprint = {2204.06125},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2023-02-11},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  pubstate = {prepublished},
  keywords = {neural_network},
  file = {/home/razor/Zotero/storage/VDEG2D9T/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf;/home/razor/Zotero/storage/NUANVRLC/2204.html}
}

@video{RaskolElitChto,
  entrysubtype = {video},
  title = {Раскол элит? Что случилось в Брянске? (Лиза Смирнова, Алексей Сахнин)},
  shorttitle = {Раскол элит?},
  url = {https://www.youtube.com/watch?v=BVuSbSO5XH0},
  urldate = {2023-03-08},
  abstract = {Подписывайтесь на Телеграм-канал "НЕВОЙНА": https://t.me/narodpvФонд "За Советское Телевидение!": 63900238 9045765500 (номер карты Сбербанка)},
  langid = {russian},
  keywords = {politics},
  file = {/home/razor/Zotero/storage/REQNJB34/BVuSbSO5XH0.html}
}

@online{RossiiRezkoVyrastet2023,
  title = {В России резко вырастет объем устаревающего жилья. Что это значит},
  date = {2023-09-25},
  url = {https://realty.rbc.ru/news/650d30fb9a7947a57708783d},
  urldate = {2024-01-23},
  abstract = {К 2030 году прирост объема устаревающего жилья составит 54 млн кв. м, а к 2040 году~— 216 млн кв. м, подсчитали в ИНП РАН. В результате общий объем такого жилья достигнет 270 млн кв. м, из них 162,8 млн кв. м предполагается выводить из эксплуатации},
  langid = {russian},
  organization = {РБК Недвижимость}
}

@online{RVPKazahstaneDlya,
  title = {{{РВП}} в {{Казахстане}} Для Граждан {{России}}: Как Получить в 2023 Году},
  url = {https://visasam.ru/emigration/pereezdsng/rvp-v-kazahstane.html},
  urldate = {2023-03-10},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/HFFYJLPQ/rvp-v-kazahstane.html}
}

@online{rybkinValueBasedDeepRL2025,
  title = {Value-{{Based Deep RL Scales Predictably}}},
  author = {Rybkin, Oleh and Nauman, Michal and Fu, Preston and Snell, Charlie and Abbeel, Pieter and Levine, Sergey and Kumar, Aviral},
  date = {2025-02-06},
  eprint = {2502.04327},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.04327},
  url = {http://arxiv.org/abs/2502.04327},
  urldate = {2025-02-08},
  abstract = {Scaling data and compute is critical to the success of machine learning. However, scaling demands predictability: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a Pareto frontier, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data. Second, we determine the optimal allocation of a total resource budget across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling behavior is enabled by first estimating predictable relationships between hyperparameters, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance.},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/RBG3B8S5/Rybkin et al. - 2025 - Value-Based Deep RL Scales Predictably.pdf;/home/razor/Zotero/storage/L9WFE78W/2502.html}
}

@inreference{ScientificMethod2023,
  title = {Scientific Method},
  booktitle = {Wikipedia},
  date = {2023-06-10T14:32:41Z},
  url = {https://en.wikipedia.org/w/index.php?title=Scientific_method&oldid=1159469923#History},
  urldate = {2023-06-22},
  abstract = {The scientific method is an empirical method for acquiring knowledge that has characterized the development of science since at least the 17th century (with notable practitioners in previous centuries; see the article history of scientific method for additional detail.) It involves careful observation, applying rigorous skepticism about what is observed, given that cognitive assumptions can distort how one interprets the observation. It involves formulating hypotheses, via induction, based on such observations; the testability of hypotheses, experimental and the measurement-based statistical testing of deductions drawn from the hypotheses; and refinement (or elimination) of the hypotheses based on the experimental findings. These are principles of the scientific method, as distinguished from a definitive series of steps applicable to all scientific enterprises.Although procedures vary from one field of inquiry to another, the underlying process is frequently the same from one field to another. The process in the scientific method involves making conjectures (hypothetical explanations), deriving predictions from the hypotheses as logical consequences, and then carrying out experiments or empirical observations based on those predictions. A hypothesis is a conjecture, based on knowledge obtained while seeking answers to the question. The hypothesis might be very specific, or it might be broad. Scientists then test hypotheses by conducting experiments or studies. A scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.The purpose of an experiment is to determine whether observations  agree with or conflict with the expectations deduced from a hypothesis.:{$\mkern1mu$}Book I,{$\mkern1mu$}[6.54] pp.372,{$\mkern1mu$}408{$\mkern1mu$} Experiments can take place anywhere from a garage to a remote mountaintop to CERN's Large Hadron Collider. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always in the same order.},
  langid = {english},
  annotation = {Page Version ID: 1159469923},
  file = {/home/razor/Zotero/storage/H28N33R5/Scientific_method.html}
}

@online{ShahurinaAnzhelikaIgorevna,
  title = {Шахурина, {{Анжелика Игоревна}} — {{FA100}}},
  url = {http://wiki.fa100.ru/index.php?title=%D0%A8%D0%B0%D1%85%D1%83%D1%80%D0%B8%D0%BD%D0%B0,_%D0%90%D0%BD%D0%B6%D0%B5%D0%BB%D0%B8%D0%BA%D0%B0_%D0%98%D0%B3%D0%BE%D1%80%D0%B5%D0%B2%D0%BD%D0%B0},
  urldate = {2025-01-17},
  file = {/home/razor/Zotero/storage/9C2TRHVJ/index.html}
}

@online{shumailovCurseRecursionTraining2024,
  title = {The {{Curse}} of {{Recursion}}: {{Training}} on {{Generated Data Makes Models Forget}}},
  shorttitle = {The {{Curse}} of {{Recursion}}},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  date = {2024-04-14},
  eprint = {2305.17493},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.17493},
  url = {http://arxiv.org/abs/2305.17493},
  urldate = {2025-02-03},
  abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as model collapse1 and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/B8XCSNNG/Shumailov et al. - 2024 - The Curse of Recursion Training on Generated Data Makes Models Forget.pdf}
}

@online{shumailovCurseRecursionTraining2024a,
  title = {The {{Curse}} of {{Recursion}}: {{Training}} on {{Generated Data Makes Models Forget}}},
  shorttitle = {The {{Curse}} of {{Recursion}}},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  date = {2024-04-14},
  eprint = {2305.17493},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.17493},
  url = {http://arxiv.org/abs/2305.17493},
  urldate = {2025-03-09},
  abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as model collapse1 and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/2SQKBKAQ/Shumailov et al. - 2024 - The Curse of Recursion Training on Generated Data Makes Models Forget.pdf}
}

@online{snellScalingLLMTestTime2024,
  title = {Scaling {{LLM Test-Time Compute Optimally}} Can Be {{More Effective}} than {{Scaling Model Parameters}}},
  author = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  date = {2024-08-06},
  eprint = {2408.03314},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.03314},
  url = {http://arxiv.org/abs/2408.03314},
  urldate = {2025-01-08},
  abstract = {Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {llm,machine_learning,prompt_engineering},
  file = {/home/razor/Zotero/storage/VUQXPV95/Snell et al. - 2024 - Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.pdf}
}

@online{SpacedRepetitionReading,
  title = {Spaced Repetition for Reading Inbox},
  url = {https://www.alexeyshmalko.com/20200830012450/},
  urldate = {2023-01-26},
  abstract = {Spaced repetition system can be used to manage reading inbox. (And that’s what I’m doing since [2020-08-09 Sun].)},
  langid = {english},
  keywords = {srs},
  file = {/home/razor/Zotero/storage/3BFTTLCZ/20200830012450.html}
}

@online{SpacedRepetitionWriting,
  title = {Spaced Repetition for Writing Inbox},
  url = {https://www.alexeyshmalko.com/20200907034608/},
  urldate = {2023-01-26},
  abstract = {The idea is similar to Spaced repetition for reading inbox, but applied to § Zettelkasten-writing inbox.},
  langid = {english},
  keywords = {srs},
  file = {/home/razor/Zotero/storage/SPE9NIZZ/20200907034608.html}
}

@article{supekarBrainHyperconnectivityChildren2013,
  title = {Brain {{Hyperconnectivity}} in {{Children}} with {{Autism}} and Its {{Links}} to {{Social Deficits}}},
  author = {Supekar, Kaustubh and Uddin, Lucina~Q. and Khouzam, Amirah and Phillips, Jennifer and Gaillard, William~D. and Kenworthy, Lauren~E. and Yerys, Benjamin~E. and Vaidya, Chandan~J. and Menon, Vinod},
  date = {2013-11},
  journaltitle = {Cell Reports},
  shortjournal = {Cell Reports},
  volume = {5},
  number = {3},
  pages = {738--747},
  issn = {22111247},
  doi = {10.1016/j.celrep.2013.10.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2211124713005706},
  urldate = {2025-02-10},
  abstract = {Autism spectrum disorder (ASD), a neurodevelopmental disorder affecting nearly 1 in 88 children, is thought to result from aberrant brain connectivity. Remarkably, there have been no systematic attempts to characterize whole-brain connectivity in children with ASD. Here, we use neuroimaging to show that there are more instances of greater functional connectivity in the brains of children with ASD in comparison to those of typically developing children. Hyperconnectivity in ASD was observed at the whole-brain and subsystems levels, across long- and short-range connections, and was associated with higher levels of fluctuations in regional brain signals. Brain hyperconnectivity predicted symptom severity in ASD, such that children with greater functional connectivity exhibited more severe social deficits. We replicated these findings in two additional independent cohorts, demonstrating again that at earlier ages, the brain of children with ASD is largely functionally hyperconnected in ways that contribute to social dysfunction. Our findings provide unique insights into brain mechanisms underlying childhood autism.},
  langid = {english},
  keywords = {autism,brain,medicine},
  file = {/home/razor/Zotero/storage/V88XIT5W/Supekar et al. - 2013 - Brain Hyperconnectivity in Children with Autism and its Links to Social Deficits.pdf}
}

@inreference{TransFat2023,
  title = {Trans Fat},
  booktitle = {Wikipedia},
  date = {2023-02-16T15:07:38Z},
  url = {https://en.wikipedia.org/w/index.php?title=Trans_fat&oldid=1139717483},
  urldate = {2023-02-27},
  abstract = {Trans fat, also called trans-unsaturated fatty acids, or trans fatty acids, is a type of unsaturated fat that occurs in foods.  Trace concentrations of trans fats occur naturally, but large amounts are found in some processed foods.  Since consumption of trans fats is unhealthy, artificial trans fats are highly regulated or banned in many nations. However, they are still widely consumed in developing nations, resulting in hundreds of thousands of deaths each year.  Serious interest has been given to determining their sources, to better avoid them.},
  langid = {english},
  keywords = {medicine},
  annotation = {Page Version ID: 1139717483},
  file = {/home/razor/Zotero/storage/H2U8ZBAP/Trans_fat.html}
}

@inproceedings{TuringCompleteTransformers2023,
  title = {Turing {{Complete Transformers}}: {{Two Transformers Are More Powerful Than One}}},
  shorttitle = {Turing {{Complete Transformers}}},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=MGWsPGogLH},
  urldate = {2024-01-10},
  abstract = {This paper presents Find+Replace transformers, a family of multi-transformer architectures that can provably do things no single transformer can, and which outperforms GPT-4 on several challenging tasks. We first establish that traditional transformers and similar architectures are not Turing Complete, while Find+Replace transformers are. Using this fact, we show how arbitrary programs can be compiled into Find+Replace transformers, potentially aiding interpretability research. We also demonstrate the superior performance of Find+Replace transformers over GPT-4 on a set of composition challenge problems. This work aims to provide a theoretical basis for multi-transformer architectures, and to encourage their further exploration.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {llm},
  file = {/home/razor/Zotero/storage/U4T2WUZJ/2023 - Turing Complete Transformers Two Transformers Are.pdf}
}

@online{TyingKnotHaskellWiki,
  title = {Tying the {{Knot}} - {{HaskellWiki}}},
  url = {https://wiki.haskell.org/Tying_the_Knot},
  urldate = {2023-01-27},
  keywords = {haskell},
  file = {/home/razor/Zotero/storage/Z5EPBQ4Y/Tying_the_Knot.html}
}

@online{uslugiKakOformitVid2022,
  title = {Как Оформить Вид На Жительство в {{Черногории}}, Основания Для Выдачи {{ВНЖ}}, Список Необходимых Документов, Этапы Получения — {{MCC Group}}: Миграционные Услуги На vc.Ru},
  shorttitle = {Как Оформить Вид На Жительство в {{Черногории}}, Основания Для Выдачи {{ВНЖ}}, Список Необходимых Документов, Этапы Получения — {{MCC Group}}},
  author = {миграционные {услуги}, MCC Group:},
  date = {2022-12-16T15:36:47+03:00},
  url = {https://vc.ru/u/1338621-mcc-group-migracionnye-uslugi/564029-kak-oformit-vid-na-zhitelstvo-v-chernogorii-osnovaniya-dlya-vydachi-vnzh-spisok-neobhodimyh-dokumentov-etapy-polucheniya},
  urldate = {2023-03-09},
  abstract = {Ожидается, что в 2025 году Черногория должна официально вступить в Евросоюз. Многие россияне уже сегодня задумываются о получении черногорского гражданства, чтобы в будущем стать полноправным жителем ЕС с возможностью безвизового въезда в разные страны мира. Огромным плюсом является тот факт, что это и самый экономичный вариант из доступных для…},
  organization = {vc.ru},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/TM9C5AQB/564029-kak-oformit-vid-na-zhitelstvo-v-chernogorii-osnovaniya-dlya-vydachi-vnzh-spisok-neobhodi.html}
}

@online{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-01-30},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {computer_science,llm,neural_network},
  file = {/home/razor/Zotero/storage/ZNHTHYX3/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/razor/Zotero/storage/8ZS4DMUE/1706.html}
}

@video{vestnikburiChEChNYaROSSIYaROSNEFT�2022,
  entrysubtype = {video},
  title = {{{ЧЕЧНЯ}}, {{РОССИЯ И}} "{{РОСНЕФТЬ}}": {{КТО КОГО КОРМИТ}}?},
  shorttitle = {{{ЧЕЧНЯ}}, {{РОССИЯ И}} "{{РОСНЕФТЬ}}"},
  editor = {{Вестник Бури}},
  editortype = {director},
  date = {2022},
  url = {https://www.youtube.com/watch?v=R3HtX_um2mY},
  urldate = {2023-02-08},
  keywords = {politics}
}

@video{vestnikburiKAKGOBLINSTAL2023,
  entrysubtype = {video},
  title = {{{КАК ГОБЛИН СТАЛ ФАШИСТОМ}}. {{Помогаем Климу Жукову}} Открыть Глаза},
  editor = {{Вестник Бури}},
  editortype = {director},
  date = {2023},
  url = {https://www.youtube.com/watch?v=-zElJhc8FYk},
  urldate = {2023-03-08},
  abstract = {Были времена, когда казалось, что просоветская риторика несовместима с ультраправыми идеями. Практика показала, что совместима. И нагляднее всего это продемонстрировал Дмитрий Пучков, который в последний год превратился из "красного путиниста" в натурального фашиста (если исходить из определения Димитрова) или просто в радикального правого (если исходить из концепций буржуазных политологов). В новом видео на примере Гоблина Андрей Рудой раскрывает феномен фашизма под красно-советской маской - феномен, который имеет серьёзный деструктивный потенциал для российского общества. P.S. Друзья, ваша поддержка пожертвованиями актуальна как никогда. Благодарим подписчиков за поддержку канала. Выход следующих видео также зависит от вас! Номер карты Сбербанка: 4276 4200 4927 7147 Номер карты Revolute 4165 9821 0210 2250 Юмани: https://yoomoney.ru/to/410014236181993 Поддержка на постоянной основе через Boosty https://boosty.to/vestnikburi Поддержка на постоянной основе через Patreon https://www.patreon.com/vestnikburi  =============================== Telergram-канал:  https://t.me/ru\_doy Вестник бури. Сайт: http://vestnikburi.com/  ВК: https://vk.com/vestnikburi  Андрей Рудой. ВК: https://vk.com/rudoy\_av  Instagram: https://www.instagram.com/rudoy1/  Twitter: https://twitter.com/RudoyA1 ================================= Встречи с подписчиками и активистами: 1. Встреча с германоязычными товарищами и теми, кто живёт в Лотарингии/Эльзасе: 31 марта, 18.00 Ресторан Woll 80 Rue des Hauteurs, 57350 Spicheren, France 2. Встреча с русскоязычными подписчиками в Сааре: 1 апреля, 14.00 Саарбрюккен, Германия, точное место уточняется ======================================== Ссылки: Биг Берия Тейп - Алло, Макрон: ~~~•~БИГ~БЕРИЯ~ТЕЙП~-~...~~  Видео про путинский империализм: ~~~•~ИМПЕРИАЛИЗМ~ПУТИН...~~  Видео про Пригожина: ~~~•~ЕВГЕНИЙ~ПРИГОЖИН~...~~  ====================================== Тайм-коды: 0:00 - Вступление, неРемарк и Клим Жуков 1:17 - Фашизм под маской "советского патриотизма" 2:45 - Марксист Дугин! 3:31 - Жуков: "Где фашистская риторика у Гоблина?" 7:00 - Объявления для зрителей 8:45 - Что такое фашизм? Споры исследователей 12:13 - Димитров против Гоблина 14:00 - Россия - фашизирующаяся страна 17:35 - Всегда ли Гоблин был таким? 18:55 - Сталин за буржуазные свободы 19:50 - Гоблин за ускоренную фашизацию России 24:30 - Оправдание репрессий через отсылки к СССР 26:10 - Гоблин за ЧВК 29:25 - Как Гоблин стал шовинистом 37:54 - Гоблин саморазоблачается 38:50 - Гоблин против леваков 40:08 - Гоблин - латентный фашист по Димитрову 41:52 - Гоблин - малолетний д*бил 46:55 - Зачем Пучков всё это делает? 48:25 - Почему окружение Гоблина не отречётся от него? 49:32 - Гоблина - под трибунал 51:06 - Музыкальный номер. Биг Берия Тейп},
  keywords = {persons,politics}
}

@video{vestnikburiKAKZAKONChIT�VOYNU2022,
  entrysubtype = {video},
  title = {{{КАК ЗАКОНЧИТЬ ВОЙНУ В}}/{{НА УКРАИНЕ}}? {{Циммервальд}} 2.0},
  shorttitle = {{{КАК ЗАКОНЧИТЬ ВОЙНУ В}}/{{НА УКРАИНЕ}}?},
  editor = {{Вестник Бури}},
  editortype = {director},
  date = {2022},
  url = {https://www.youtube.com/watch?v=-rmT9i6JiZI},
  urldate = {2023-01-18},
  keywords = {politics}
}

@video{vestnikburiKOMUVYGODNAVOYNA2022,
  entrysubtype = {video},
  title = {{{КОМУ ВЫГОДНА ВОЙНА В}}/{{НА УКРАИНЕ}}? (Feat. {{Даниил Григорьев}})},
  shorttitle = {{{КОМУ ВЫГОДНА ВОЙНА В}}/{{НА УКРАИНЕ}}?},
  editor = {{Вестник Бури}},
  editortype = {director},
  date = {2022},
  url = {https://www.youtube.com/watch?v=xtgO8etwbDs},
  urldate = {2023-01-18},
  keywords = {politics}
}

@video{vestnikburiPUTINSKIYKAPITALIZMKak2020,
  entrysubtype = {video},
  title = {{{ПУТИНСКИЙ КАПИТАЛИЗМ}}. {{Как Путин}} "Спас" {{Россию}} От 90-х},
  editor = {{Вестник Бури}},
  editortype = {director},
  date = {2020},
  url = {https://www.youtube.com/watch?v=ddDpdfXlMxY},
  urldate = {2023-02-08},
  keywords = {politics}
}

@online{VizaMongoliyuNuzhna,
  title = {Виза в {{Монголию}}: Нужна Ли Для Россиян в 2023 Году, Правила Въезда в Страну},
  url = {https://visasam.ru/oformlenie/free/nuzhna-li-visa-v-mongoliyu.html#i-2},
  urldate = {2023-03-09},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/YVDTQAYS/nuzhna-li-visa-v-mongoliyu.html}
}

@online{VlastiMongoliiZayavili2022,
  title = {Власти Монголии заявили о намерении выдавать вид на жительство всем россиянам},
  date = {2022-10-01},
  url = {https://www.forbes.ru/society/478639-vlasti-mongolii-zaavili-o-namerenii-vydavat-vid-na-zitel-stvo-vsem-rossianam},
  urldate = {2023-03-09},
  abstract = {Монголия будет выдавать вид на жительство всем россиянам, которые обратятся в иммиграционное агентство, заявил чиновник этого ведомства. После объявления в России частичной мобилизации, в Монголию въехало более 6200 россиян},
  langid = {russian},
  organization = {Forbes.ru},
  keywords = {emigration},
  file = {/home/razor/Zotero/storage/RRI8BXQ9/478639-vlasti-mongolii-zaavili-o-namerenii-vydavat-vid-na-zitel-stvo-vsem-rossianam.html}
}

@online{wallossekAMDUEFIWhat2020,
  title = {{{AMD UEFI Inside}}: {{What}} Is Really behind {{AGESA}}, the {{PSP}} ({{Platform Security Processor}}) and Especially {{ComboPI}}?},
  shorttitle = {{{AMD UEFI Inside}}},
  author = {Wallossek, Igor},
  date = {2020-06-24T04:00+00:00},
  url = {https://www.igorslab.de/en/inside-amd-bios-what-is-really-hidden-behind-agesa-the-psp-platform-security-processor-and-the-numbers-of-combo-pi/},
  urldate = {2023-04-14},
  abstract = {Since there are always questions and some things are often confused, we will give you some insights into AMD-UEFI, what is colloquially called "the BIOS" (although it is no longer correct).},
  langid = {american},
  organization = {igor'sLAB},
  keywords = {amd},
  file = {/home/razor/Zotero/storage/HVQRDCKW/inside-amd-bios-what-is-really-hidden-behind-agesa-the-psp-platform-security-processor-and-the-.html}
}

@online{wangMathPileBillionTokenScalePretraining2024,
  title = {{{MathPile}}: {{A Billion-Token-Scale Pretraining Corpus}} for {{Math}}},
  shorttitle = {{{MathPile}}},
  author = {Wang, Zengzhi and Li, Xuefeng and Xia, Rui and Liu, Pengfei},
  date = {2024-10-29},
  eprint = {2312.17120},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.17120},
  url = {http://arxiv.org/abs/2312.17120},
  urldate = {2025-02-10},
  abstract = {High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MATHPILE , a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of “less is more”, firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MATHPILE to boost language models’ mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at https://github.com/GAIR-NLP/MathPile/).},
  langid = {english},
  pubstate = {prepublished},
  keywords = {AI,llm,pretraining},
  file = {/home/razor/Zotero/storage/MNG2UK2Q/Wang et al. - 2024 - MathPile A Billion-Token-Scale Pretraining Corpus for Math.pdf}
}

@online{wangMixtureofAgentsEnhancesLarge2024,
  title = {Mixture-of-{{Agents Enhances Large Language Model Capabilities}}},
  author = {Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
  date = {2024-06-07},
  eprint = {2406.04692},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2406.04692},
  urldate = {2024-06-24},
  abstract = {Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1\% compared to 57.5\% by GPT-4 Omni.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {llm,prompt_engineering},
  file = {/home/razor/Zotero/storage/8EI9GYUW/Wang et al. - 2024 - Mixture-of-Agents Enhances Large Language Model Ca.pdf}
}

@online{wangSelfInstructAligningLanguage2023,
  title = {Self-{{Instruct}}: {{Aligning Language Models}} with {{Self-Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  date = {2023-05-25},
  eprint = {2212.10560},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2024-07-11},
  abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/MW8PCEQN/Wang et al. - 2023 - Self-Instruct Aligning Language Models with Self-.pdf}
}

@online{wangTokenFormerRethinkingTransformer2024,
  title = {{{TokenFormer}}: {{Rethinking Transformer Scaling}} with {{Tokenized Model Parameters}}},
  shorttitle = {{{TokenFormer}}},
  author = {Wang, Haiyang and Fan, Yue and Naeem, Muhammad Ferjad and Xian, Yongqin and Lenssen, Jan Eric and Wang, Liwei and Tombari, Federico and Schiele, Bernt},
  date = {2024-10-30},
  eprint = {2410.23168},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.23168},
  url = {http://arxiv.org/abs/2410.23168},
  urldate = {2024-11-06},
  abstract = {Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at \textbackslash url\{https://github.com/Haiyang-W/TokenFormer\}.},
  pubstate = {prepublished},
  keywords = {llm,neural_network},
  file = {/home/razor/Zotero/storage/LMK9H4T8/Wang et al. - 2024 - TokenFormer Rethinking Transformer Scaling with T.pdf;/home/razor/Zotero/storage/3WH52TD5/2410.html}
}

@article{weiChainofThoughtPromptingElicits,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  langid = {english},
  file = {/home/razor/Zotero/storage/G48HHCZG/Wei et al. - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@online{WritingSnippets,
  title = {Writing Snippets},
  url = {https://joaotavora.github.io/yasnippet/snippet-development.html},
  urldate = {2023-01-19},
  keywords = {emacs,snippets},
  file = {/home/razor/Zotero/storage/MYRQHEBZ/snippet-development.html}
}

@video{XXRossiyskiyObligacionnyy,
  entrysubtype = {video},
  title = {{{XX Российский}} Облигационный Конгресс - {{Google Chrome}} 2022-12-09 12-11-39},
  url = {https://cloud.mail.ru/public/DzLh/qpnRSAZZo},
  urldate = {2023-01-20},
  abstract = {Облако Mail.ru - это ваше персональное надёжное хранилище в интернете.},
  keywords = {economy},
  file = {/home/razor/Zotero/storage/MKVE7Z5U/qpnRSAZZo.html}
}

@online{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  date = {2023-12-03},
  eprint = {2305.10601},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.10601},
  urldate = {2024-06-21},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Thought” approach to prompting language models, and enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/razor/Zotero/storage/CIJHTCVL/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf}
}

@online{yeLIMOLessMore2025,
  title = {{{LIMO}}: {{Less}} Is {{More}} for {{Reasoning}}},
  shorttitle = {{{LIMO}}},
  author = {Ye, Yixin and Huang, Zhen and Xiao, Yang and Chern, Ethan and Xia, Shijie and Liu, Pengfei},
  date = {2025-02-05},
  eprint = {2502.03387},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.03387},
  url = {http://arxiv.org/abs/2502.03387},
  urldate = {2025-02-10},
  abstract = {We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data ({$>$}100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1\% accuracy on AIME and 94.8\% on MATH, improving from previous SFT-based models' 6.5\% and 59.2\% respectively, while only using 1\% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5\% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {AI,llm,math,prompt_engineering,reasoning},
  file = {/home/razor/Zotero/storage/BH5UC6CU/Ye et al. - 2025 - LIMO Less is More for Reasoning.pdf}
}

@online{ZemletryasenieMagnitudoyProizoshlo2023,
  title = {Землетрясение магнитудой 5,9 произошло в Киргизии, толчки ощущались в Алма-Ате},
  date = {2023-02-27},
  url = {https://www.mk.ru/incident/2023/02/27/zemletryasenie-magnitudoy-59-proizoshlo-v-kirgizii-tolchki-oshhushhalis-v-almaate.html},
  urldate = {2023-03-09},
  abstract = {Землетрясение магнитудой 5,9 произошло на территории Киргизии, его ощутили жители казахстанского крупного города Алма-Аты, информирует сейсмологическая опытно-методическая экспедиция казахстанского министерства по чрезвычайным ситуациям},
  langid = {russian},
  keywords = {emigration,geology},
  file = {/home/razor/Zotero/storage/ZHS8J3WV/zemletryasenie-magnitudoy-59-proizoshlo-v-kirgizii-tolchki-oshhushhalis-v-almaate.html}
}

@online{zhouLIMALessMore2023,
  title = {{{LIMA}}: {{Less Is More}} for {{Alignment}}},
  shorttitle = {{{LIMA}}},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  date = {2023-05-18},
  eprint = {2305.11206},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.11206},
  url = {http://arxiv.org/abs/2305.11206},
  urldate = {2025-02-10},
  abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {AI,fine_tuning,llm,prompt_engineering},
  file = {/home/razor/Zotero/storage/TT3VJM5S/Zhou et al. - 2023 - LIMA Less Is More for Alignment.pdf}
}
